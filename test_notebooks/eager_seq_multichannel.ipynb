{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Documents/venv3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "tf.set_random_seed(230)\n",
    "print(tf.__version__)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.seq_multichannel_biogas import hparams\n",
    "hparams = hparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader.biogas import *\n",
    "get_fields = 'AT305 FT305'\n",
    "dataset, dataset_val, dataset_test = load_data(hparams, mode=get_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(units, name):\n",
    "    # Use CuDNNGRU is GPU is available (provides a 3x speedup than GRU)\n",
    "    if tf.test.is_gpu_available():\n",
    "        return tf.keras.layers.CuDNNGRU(units, \n",
    "                                        return_sequences=True, \n",
    "                                        return_state=True, \n",
    "                                        recurrent_activation='relu',\n",
    "                                        recurrent_initializer='glorot_uniform',\n",
    "                                        name=name)\n",
    "    else:\n",
    "        return tf.keras.layers.GRU(units, \n",
    "                                   return_sequences=True, \n",
    "                                   return_state=True, \n",
    "                                   recurrent_activation='relu', \n",
    "                                   recurrent_initializer='glorot_uniform',\n",
    "                                   name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units, name=None):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units, name=name + '_W1')\n",
    "        self.W2 = tf.keras.layers.Dense(units, name=name + '_W2')\n",
    "        self.V = tf.keras.layers.Dense(1, name=name + '_V')\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, 64, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        context_vector = tf.expand_dims(context_vector, 1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, output_dim, layers, units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.layer_s = layers\n",
    "        self.units = units\n",
    "        self.cells = [gru(self.units, 'encoder_gru_%i' % i) for i in range(layers)]\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        outputs, states = [], []\n",
    "        output = x\n",
    "        \n",
    "        for cell in self.cells:\n",
    "            output, state = cell(output, initial_state = hidden)\n",
    "            outputs.append(output)\n",
    "            states.append(state)\n",
    "\n",
    "        return outputs, states\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, output_dim, num_layers, neurons_unit, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_dim   = output_dim\n",
    "        self.num_layers   = num_layers\n",
    "        self.neurons_unit = neurons_unit\n",
    "        self.batch_size   = batch_size\n",
    "        \n",
    "        self.cells = [gru(neurons_unit, 'decoder_gru_%i' % i) for i in range(num_layers)]\n",
    "        self.attentions = [BahdanauAttention(neurons_unit, 'decoder_attn_%i' % i) for i in range(num_layers)]\n",
    "        self.fc_out = tf.keras.layers.Dense(output_dim, activation='relu', name='decoder_affine_out')\n",
    "                \n",
    "    def call(self, x, dec_states, enc_outputs, mask):\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        dec_output = x\n",
    "        states = []\n",
    "        for layer, cell in enumerate(self.cells):\n",
    "            context_vector, _ = self.attentions[layer](enc_outputs[layer], dec_states[layer])\n",
    "        \n",
    "            # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "            dec_output = tf.concat([context_vector, dec_output], axis=-1)\n",
    "\n",
    "            # passing the concatenated vector to the GRU\n",
    "            dec_output, dec_state = self.cells[layer](dec_output, initial_state=dec_states[layer])\n",
    "            dec_states.append(dec_state)\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc_out(dec_output) * mask\n",
    "        \n",
    "        return x, dec_states\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(tf.keras.Model):\n",
    "    def __init__(self, output_dim, num_layers, neurons_unit, batch_size):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.output_dim   = output_dim\n",
    "        self.num_layers   = num_layers\n",
    "        self.neurons_unit = neurons_unit\n",
    "        self.batch_size   = batch_size\n",
    "\n",
    "        self.encoder = Encoder(output_dim, num_layers, neurons_unit, batch_size)\n",
    "        self.decoder = Decoder(output_dim, num_layers, neurons_unit, batch_size)\n",
    "    \n",
    "    def call(self, inp, mask):\n",
    "        hidden = self.encoder.initialize_hidden_state()\n",
    "        enc_output, enc_hidden = self.encoder(inp, hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.concat((inp[:,0,:], inp[:,-1,-self.output_dim:]), axis=1)\n",
    "        dec_input = tf.expand_dims(dec_input, 1)\n",
    "\n",
    "        for t in range(0, inp.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            prediction, dec_hidden = self.decoder(dec_input, dec_hidden, enc_output, mask[:,t:t+1])\n",
    "\n",
    "            # Concatenate with prediction from previous time step\n",
    "            dec_input = tf.concat((tf.expand_dims(inp[:,t,:],1), prediction), axis=2)\n",
    "            \n",
    "            if t == 0:\n",
    "                predictions = prediction\n",
    "            else:\n",
    "                predictions = tf.concat([predictions, prediction], axis=1)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "loss_function: Build out MSE loss function with parameter regularisation\n",
    "\n",
    "train_model: Runs the minibatch training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a training session and print training statistics and model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "> <ipython-input-7-6fe641b001e2>(19)call()\n",
      "-> for layer, cell in enumerate(self.cells):\n"
     ]
    }
   ],
   "source": [
    "import trainers.tf_eager_trainer as trainer\n",
    "\n",
    "# Run a training batch\n",
    "tf.set_random_seed(231) # Set seed\n",
    "\n",
    "# Initialise model and optimiser\n",
    "model = EncoderDecoder(hparams.output_channels, hparams.num_layers, hparams.neurons_unit, hparams.batch_size)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = hparams.learning_rate)\n",
    "\n",
    "# Start training run\n",
    "loss, accuracy, run_time, stats = trainer.train_model(model, optimizer, dataset, hparams, epochs=2, verbose=True)\n",
    "\n",
    "# Display results\n",
    "print('Loss {:.4f} Accuracy {:.4f} Time {:.4f}'.format(loss * 100, accuracy * 100, run_time))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.array(stats)[:,:2])\n",
    "plt.show()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOT CHECKED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_POINT = 0\n",
    "total_accuracy = []\n",
    "\n",
    "for i, (inp, target, mask) in enumerate(dataset_val):\n",
    "    forecast = []\n",
    "\n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    enc_output, enc_hidden = encoder(inp[:, START_POINT:], hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.concat((inp[:,0,:], tf.expand_dims(inp[:,-1,-1], 1)), axis=1)\n",
    "    dec_input = tf.expand_dims(dec_input, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, target.shape[1]):\n",
    "        # passing enc_output to the decoder\n",
    "        predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_output, mask[:,t:t+1])\n",
    "        forecast.append(np.squeeze(predictions))\n",
    "\n",
    "        # using teacher forcing - adapted to feed predictions\n",
    "        dec_input = tf.concat((tf.expand_dims(inp[:,t,:],1), predictions), axis=2)\n",
    "\n",
    "    accuracy = (np.array(forecast).T - target[:,:60,0]) * x_max\n",
    "    accuracy = accuracy ** 2 / x_var\n",
    "    accuracy = np.mean(accuracy, axis=0)\n",
    "    \n",
    "    total_accuracy.append(accuracy)\n",
    "\n",
    "model_accuracy.append((hparams.in_seq_len, np.mean(total_accuracy, axis=0)))\n",
    "    \n",
    "print('Total Accuracy: %.4f' % np.mean(total_accuracy))\n",
    "plt.plot(np.mean(total_accuracy, axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for t, data in model_accuracy:\n",
    "    plt.plot(data, label = 'Training Set: %i hrs' % t)\n",
    "plt.legend()\n",
    "plt.xlabel('Forecast Horizon (hours)')\n",
    "plt.ylabel('Relative Squared Error')\n",
    "plt.title('Encoder-Decoder Network')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Hyperparameters\n",
    "#CHECKED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "models, stats = [], []\n",
    "\n",
    "lr    = np.log10(hparams.learning_rate)\n",
    "nodes = hparams.neurons_unit\n",
    "\n",
    "#for _ in range(25):\n",
    "#   lr = np.random.uniform(-8, -1, 1):\n",
    "#   nodes = np.floor(2 ** np.random.randint((3,9))):\n",
    "#for nodes in np.floor(2 ** np.arange(3,9)):\n",
    "for lr in range(-8, 0, 1):\n",
    "    lr /= 2\n",
    "\n",
    "    # Run a training batch\n",
    "    tf.set_random_seed(231) # Set seed\n",
    "\n",
    "    # Initialise model and optimiser\n",
    "    model_ = EncoderDecoder(hparams.output_dim, hparams.num_layers, hparams.neurons_unit, hparams.batch_size)\n",
    "    optimizer_ = tf.train.AdamOptimizer(learning_rate = 10 ** lr)\n",
    "\n",
    "    # Start training run\n",
    "    loss, accuracy, run_time, stat = \\\n",
    "        trainer.train_model(model_, optimizer_, dataset, hparams, epochs = 2, verbose=False)\n",
    "    print('Learning Rate {:.4f} Nodes {} Loss {:.4f} Accuracy {:.4f} Time {:.1f}'.format(\\\n",
    "                lr, nodes, loss*100, accuracy * 100, run_time))\n",
    "    \n",
    "    models.append(model_)\n",
    "    stats.append(stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# empty string to store our results\n",
    "START_POINT = 0\n",
    "forecast = []\n",
    "\n",
    "_, (inp, target, mask) = enumerate(dataset).__next__()\n",
    "\n",
    "hidden = encoder_.initialize_hidden_state()\n",
    "enc_output, enc_hidden = encoder_(inp[:, START_POINT:], hidden)\n",
    "\n",
    "dec_hidden = enc_hidden\n",
    "\n",
    "dec_input = tf.concat((inp[:,0,:], tf.expand_dims(inp[:,-1,-1], 1)), axis=1)\n",
    "dec_input = tf.expand_dims(dec_input, 1)\n",
    "\n",
    "# Teacher forcing - feeding the target as the next input\n",
    "for t in range(1, target.shape[1]):\n",
    "    # passing enc_output to the decoder\n",
    "    predictions, dec_hidden = decoder_(dec_input, dec_hidden, enc_output, mask[:,t:t+1])\n",
    "    forecast.append(np.squeeze(predictions))\n",
    "\n",
    "    _, _ = loss_function(target[:, t:t+1], predictions, encoder_.variables + decoder_.variables)\n",
    "\n",
    "    # using teacher forcing - adapted to feed predictions\n",
    "    dec_input = tf.concat((tf.expand_dims(inp[:,t,:],1), predictions), axis=2)\n",
    "\n",
    "forecast = np.array(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# empty string to store our results\n",
    "TARGET_INDEX = 12\n",
    "START_POINT = 0\n",
    "\n",
    "# show target sequence\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.plot(np.squeeze(forecast[:,TARGET_INDEX]), label='forecast')\n",
    "plt.plot(np.squeeze(target[TARGET_INDEX]), label='target')\n",
    "plt.plot(np.mean(inp, axis=2)[TARGET_INDEX], 'bo', label='5yr average')\n",
    "for i in range(hparams.input_dim):\n",
    "    plt.plot(np.array(inp)[TARGET_INDEX,:,i], '--', label='%i years prior' % (5-i))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all sequences\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.plot(np.sum(forecast, axis=1), label='forecast')\n",
    "plt.plot(np.sum(target, axis=0), label='target')\n",
    "plt.plot(np.sum(inp, axis=0).mean(axis=1), 'bo', label='5yr average')\n",
    "for i in range(hparams.input_dim):\n",
    "    plt.plot(np.sum(inp, axis=0)[:,i], '--', label='%i years prior' % (5-i))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "for i in range(0,100):\n",
    "    ax = plt.subplot(10,10,i+1)\n",
    "    ax.plot(np.squeeze(forecast[:,i]), label='forecast')\n",
    "    ax.plot(np.squeeze(target[i]), label='target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_ = np.sum((forecast.T - np.squeeze(target)[:,:-1]) ** 2, axis=1)\n",
    "l_.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.mean((forecast.T*8.15 - target[:,1:,0]*8.15) ** 2 / dataset.var(), axis=0)\n",
    "plt.plot(loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from examples contained in the official Tensorflow github repo;\n",
    "<href>https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb</href>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
