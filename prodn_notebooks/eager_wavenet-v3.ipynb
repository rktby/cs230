{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "tf.set_random_seed(230)\n",
    "print(tf.__version__)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('batch_size', 800), ('datagen', 'biogas'), ('dilation_channels', 16), ('dilations', [1, 2, 4, 8, 16, 32, 64]), ('filter_width', 2), ('in_seq_len', 258), ('initial_filter_width', 2), ('input_channels', 1), ('input_dim', 1), ('lambd', 1e-10), ('learning_rate', 0.01), ('logs_path', '/tmp/tensorflow_logs'), ('lr_decay', 0.995), ('norm_epsilon', 1e-12), ('out_seq_len', 24), ('output_channels', 1), ('output_dim', 1), ('quantization_channels', 100), ('residual_channels', 16), ('sample_rate', 24), ('scalar_input', False), ('skip_channels', 32), ('test_split', 0.1), ('train_split', 0.8), ('use_biases', True), ('val_split', 0.1)]\n"
     ]
    }
   ],
   "source": [
    "from configs.wavenet_biogas import hparams\n",
    "hparams = hparams()\n",
    "print(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams.in_seq_len = 3 * 128 + 2\n",
    "hparams.out_seq_len = 1\n",
    "hparams.batch_size = 400\n",
    "hparams.dilations = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "hparams.learning_rate = 1e-4\n",
    "hparams.quantization_channels = 256\n",
    "hparams.residual_channels = 32\n",
    "hparams.dilation_channels = 128\n",
    "hparams.skip_channels = 128\n",
    "hparams.lambd = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14470, 1000)\n",
      "Loading Data - Mode: midi\n"
     ]
    }
   ],
   "source": [
    "from data_loader.prodn import *\n",
    "get_fields = 'midi'\n",
    "dataset, dataset_val, dataset_test = load_data(hparams, mode=get_fields, normalise='global_max_min')\n",
    "inp, target, mask, x_max, x_min = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jitter(inp, target):\n",
    "    delta = tf.random.uniform((inp.shape[0],1,1), -0.05, 0.05)\n",
    "    inp = inp + delta\n",
    "    target = target + delta\n",
    "\n",
    "    for _ in range(3):\n",
    "        delta = tf.reduce_min(tf.minimum(tf.concat([inp, target], 1),0), axis=1, keepdims=True)\n",
    "        inp = inp - 2 * delta\n",
    "        target = target - 2 * delta\n",
    "\n",
    "        delta = tf.reduce_max(tf.maximum(tf.concat([inp, target], 1) - 1,0), axis=1, keepdims=True)\n",
    "        inp = inp - 2 * delta\n",
    "        target = target - 2 * delta\n",
    "        \n",
    "    return inp, target\n",
    "\n",
    "def create_gaussian_labels(inp):\n",
    "    labels = tf.cast(hparams.quantization_channels * (inp), tf.int32)\n",
    "    labels = tf.one_hot(labels, (hparams.quantization_channels+1))\n",
    "    labels = tf.transpose(labels, [0,1,3,2])\n",
    "    labels = gaussian_filter(labels)\n",
    "    labels = tf.squeeze(labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNet(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    https://github.com/ibab/tensorflow-wavenet\n",
    "    \"\"\"\n",
    "    def __init__(self, params):\n",
    "        super(WaveNet, self).__init__()\n",
    "        \n",
    "        self.params = params\n",
    "        self.outputs = []\n",
    "        self.stack = []\n",
    "        self.receptive_field = params['initial_filter_width'] + \\\n",
    "                               (params['filter_width'] - 1) * sum(params['dilations'])\n",
    "        \n",
    "        # Pre-process the input with a regular convolution\n",
    "        self.pre_process_layer = self._create_causal_conv('pre-process', params, 1, 'linear', True)\n",
    "        \n",
    "        # Create dilation layers\n",
    "        current_layer = self.pre_process_layer\n",
    "        with tf.name_scope('dilated_stack'):\n",
    "            for layer_index, dilation in enumerate(params['dilations']):\n",
    "                with tf.name_scope('layer{}'.format(layer_index)):\n",
    "                    dilation_layer = self._create_dilation_layer(layer_index, dilation, params)\n",
    "                    self.stack.append(dilation_layer)\n",
    "                    \n",
    "        # Post-process the outputs\n",
    "        with tf.name_scope('post_process'):\n",
    "            with tf.name_scope('conv_1'):\n",
    "                self.post_process1 = tf.keras.layers.Conv1D(params['skip_channels'], 1,\n",
    "                                        padding='same', use_bias=params['use_biases'],\n",
    "                                        activation='relu')\n",
    "            with tf.name_scope('conv_2'):\n",
    "                self.post_process2 = tf.keras.layers.Conv1D(params['quantization_channels'], 1,\n",
    "                                        padding='same', use_bias=params['use_biases'],\n",
    "                                        activation='linear')\n",
    "\n",
    "                    \n",
    "    def _create_causal_conv(self, name, params, dilation, activation, is_initial_layer):\n",
    "        with tf.name_scope(name):\n",
    "            filter_width = params['initial_filter_width'] if is_initial_layer else params['filter_width']\n",
    "\n",
    "            return tf.keras.layers.Conv1D(params['residual_channels'], filter_width,\n",
    "                                            padding='valid', use_bias=params['use_biases'],\n",
    "                                            dilation_rate = dilation, activation=activation)\n",
    "    \n",
    "    def _create_dilation_layer(self, layer_index, dilation, params):\n",
    "        \n",
    "        block = {}\n",
    "\n",
    "        # Dilated convolutions\n",
    "        block['filter'] = self._create_causal_conv('filter', params, dilation, 'sigmoid', False)\n",
    "        block['gate']   = self._create_causal_conv('gate', params, dilation, 'tanh',    False)\n",
    "\n",
    "        # 1x1 conv for residual and skip outputs\n",
    "        with tf.name_scope('dense'):\n",
    "            block['dense']  = tf.keras.layers.Conv1D(params['dilation_channels'], 1, activation='linear',\n",
    "                                        padding='same', use_bias=params['use_biases'])\n",
    "        with tf.name_scope('skip'):\n",
    "            block['skip']  = tf.keras.layers.Conv1D(params['skip_channels'], 1, activation='relu',\n",
    "                                        padding='same', use_bias=params['use_biases'])\n",
    "        \n",
    "        return block\n",
    "    \n",
    "    def call(self, inp, mask):\n",
    "        self.skip_connections = []\n",
    "        output_width = tf.shape(inp)[1] - self.receptive_field + 1\n",
    "        \n",
    "        x = self.pre_process_layer(inp)\n",
    "\n",
    "        for layer_index, dilation in enumerate(self.params['dilations']):\n",
    "            block = self.stack[layer_index]\n",
    "\n",
    "            # Dilation connection\n",
    "            x_conv = block['filter'](x) * block['gate'](x)\n",
    "            \n",
    "            # Skip connection\n",
    "            skip_cut = tf.shape(x_conv)[1] - output_width\n",
    "            x_skip = tf.slice(x_conv, [0, skip_cut, 0], [-1, -1, -1])\n",
    "            x_skip = block['skip'](x_skip)\n",
    "            self.skip_connections.append(x_skip)\n",
    "\n",
    "            # Dense residual connection\n",
    "            x_dens = block['dense'](x_conv)\n",
    "            input_cut = tf.shape(x)[1] - tf.shape(x_dens)[1]\n",
    "            x = x_dens + tf.slice(x, [0, input_cut, 0], [-1, -1, -1])\n",
    "\n",
    "        total = sum(self.skip_connections)\n",
    "        total = self.post_process1(total)\n",
    "        total = self.post_process2(total)\n",
    "\n",
    "        return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNet(tf.keras.Model):\n",
    "    '''\n",
    "    https://github.com/NVIDIA/nv-wavenet/blob/master/pytorch/wavenet.py\n",
    "    '''\n",
    "    def __init__(self, params):\n",
    "        super(WaveNet, self).__init__()\n",
    "\n",
    "        self.num_layers = len(params.dilations)\n",
    "        self.max_dilation = max(params.dilations)\n",
    "        self.residual_channels = params.residual_channels \n",
    "        self.skip_channels = params.skip_channels\n",
    "        self.quantization_channels = params.quantization_channels\n",
    "        self.receptive_field = params.initial_filter_width - 1 + \\\n",
    "                               (params.filter_width - 1) * sum(params.dilations)\n",
    "        \n",
    "        self.dilate_layers = []\n",
    "        self.res_layers = []\n",
    "        self.skip_layers = []\n",
    "        \n",
    "        self.embed = tf.keras.layers.Embedding(self.quantization_channels + 1, self.residual_channels,\n",
    "                                               name='embedding')\n",
    "        self.conv_out = tf.keras.layers.Conv1D(int(self.quantization_channels ** 0.5), 1, use_bias=False,\n",
    "                                               activation='relu', name='conv_out')\n",
    "        self.conv_end = tf.keras.layers.Conv1D(self.quantization_channels + 1, 1, use_bias=False,\n",
    "                                               name='conv_end')\n",
    "            # Need an extra channel to create enough buckets e.g. [0-100] Needs 101 channels\n",
    "\n",
    "        loop_factor = np.floor(np.log2(self.max_dilation)) + 1\n",
    "        for ix in range(self.num_layers):\n",
    "            dilation = int(2 ** (ix % loop_factor))\n",
    "            \n",
    "            layer = 'layer{}'.format(ix)\n",
    "            in_layer = tf.keras.layers.Conv1D(2 * params.residual_channels, kernel_size=2, padding='causal',\n",
    "                                          dilation_rate=dilation, use_bias=params.use_biases,\n",
    "                                          name=layer+'_dilated_conv')\n",
    "            self.dilate_layers.append(in_layer)\n",
    "\n",
    "            # last one is not necessary\n",
    "            if ix < self.num_layers - 1:\n",
    "                res_layer = tf.keras.layers.Conv1D(params.residual_channels, 1, use_bias=params.use_biases,\n",
    "                                                name=layer+'_residual_conv')\n",
    "                self.res_layers.append(res_layer)\n",
    "\n",
    "            skip_layer = tf.keras.layers.Conv1D(params.skip_channels, 1, use_bias=params.use_biases,\n",
    "                                                name=layer+'_skip_conv')\n",
    "            self.skip_layers.append(skip_layer)\n",
    "\n",
    "    def call(self, inp, mask):\n",
    "        \n",
    "        assert(np.max(inp) <= 1.)\n",
    "        assert(np.min(inp) >= 0.)\n",
    "        inp = tf.cast(model.quantization_channels * tf.squeeze(inp), tf.int16)\n",
    "        inp = self.embed(inp)\n",
    "       \n",
    "        for i in range(self.num_layers):\n",
    "            in_act = self.dilate_layers[i](inp)\n",
    "            t_act = tf.tanh(in_act[:, :, :self.residual_channels])\n",
    "            s_act = tf.sigmoid(in_act[:, :, self.residual_channels:])\n",
    "            acts = t_act * s_act\n",
    "            if i < len(self.res_layers):\n",
    "                res_acts = self.res_layers[i](acts)\n",
    "            inp = res_acts + inp\n",
    "\n",
    "            if i == 0:\n",
    "                output = self.skip_layers[i](acts)\n",
    "            else:\n",
    "                output = self.skip_layers[i](acts) + output\n",
    "\n",
    "        output = tf.nn.relu(output)\n",
    "        output = self.conv_out(output)\n",
    "        output = tf.nn.relu(output)\n",
    "        output = self.conv_end(output)\n",
    "        \n",
    "        # Remove last probability and replace first value with 0's because we don't know\n",
    "\n",
    "        last = output[:, -1:]\n",
    "        output = output[:, :-1]\n",
    "        first = last * 0.0\n",
    "        output = tf.concat((first, output), 1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNet(tf.keras.Model):\n",
    "    '''\n",
    "    https://github.com/NVIDIA/nv-wavenet/blob/master/pytorch/wavenet.py\n",
    "    Downsizing residual connections with linear atrous connnections\n",
    "    '''\n",
    "    def __init__(self, params):\n",
    "        super(WaveNet, self).__init__()\n",
    "\n",
    "        self.num_layers = len(params['dilations'])\n",
    "        self.max_dilation = max(params['dilations'])\n",
    "        self.residual_channels = params['residual_channels'] \n",
    "        self.skip_channels = params['skip_channels'] \n",
    "        self.out_channels = params['quantization_channels']\n",
    "        \n",
    "        self.receptive_field = params['initial_filter_width'] - 1 + \\\n",
    "                               (params['filter_width'] - 1) * sum(params['dilations'])\n",
    "        \n",
    "        self.dilate_layers = []\n",
    "        self.res_layers = []\n",
    "        self.skip_layers = []\n",
    "        \n",
    "        self.embed = tf.keras.layers.Embedding(self.out_channels, self.residual_channels)\n",
    "        self.conv_out = tf.keras.layers.Conv1D(params['dilation_channels'] , 1, use_bias=False, activation='relu')\n",
    "        self.conv_end = tf.keras.layers.Conv1D(self.out_channels, 1, use_bias=False)\n",
    "\n",
    "        loop_factor = np.floor(np.log2(self.max_dilation)) + 1\n",
    "        for i in range(self.num_layers):\n",
    "            dilation = int(2 ** (i % loop_factor))\n",
    "            \n",
    "            # Kernel size is 2 in nv-wavenet\n",
    "            in_layer = tf.keras.layers.Conv1D(2 * params['residual_channels'], kernel_size=2,\n",
    "                                              dilation_rate=dilation, use_bias=params['use_biases'])\n",
    "            self.dilate_layers.append(in_layer)\n",
    "\n",
    "            # last one is not necessary\n",
    "            if i < self.num_layers - 1:\n",
    "                res_layer = [tf.keras.layers.Conv1D(params['residual_channels'], 1, use_bias=params['use_biases']),\n",
    "                             tf.keras.layers.Conv1D(params['residual_channels'], kernel_size=2,\n",
    "                                              dilation_rate=dilation, use_bias=params['use_biases'])]\n",
    "                self.res_layers.append(res_layer)\n",
    "\n",
    "            if i == 0:\n",
    "                skip_layer = [tf.keras.layers.Conv1D(params['skip_channels'], 1, use_bias=params['use_biases'])]\n",
    "            else:\n",
    "                skip_layer = [tf.keras.layers.Conv1D(params['skip_channels'], 1, use_bias=params['use_biases']),\n",
    "                              tf.keras.layers.Conv1D(params['skip_channels'], kernel_size=2,\n",
    "                                              dilation_rate=dilation, use_bias=params['use_biases'])]\n",
    "\n",
    "            self.skip_layers.append(skip_layer)\n",
    "\n",
    "    def call(self, inp, mask):\n",
    "\n",
    "        assert(np.max(inp) <= 1.)\n",
    "        assert(np.min(inp) >= 0.)\n",
    "        inp = self.embed(tf.cast(self.quantization_channels * (tf.squeeze(inp)), tf.int16))\n",
    "       \n",
    "        for i in range(self.num_layers):\n",
    "            in_act = self.dilate_layers[i](inp)\n",
    "            t_act = tf.tanh(in_act[:, :, :self.residual_channels])\n",
    "            s_act = tf.sigmoid(in_act[:, :, self.residual_channels:])\n",
    "            acts = t_act * s_act\n",
    "            if i < len(self.res_layers):\n",
    "                res_acts = self.res_layers[i][0](acts)\n",
    "                inp = res_acts + self.res_layers[i][1](inp)\n",
    "\n",
    "            if i == 0:\n",
    "                output = self.skip_layers[i][0](acts)\n",
    "            else:\n",
    "                output = self.skip_layers[i][0](acts) + self.skip_layers[i][1](output)\n",
    "\n",
    "        output = tf.nn.relu(output)\n",
    "        output = self.conv_out(output)\n",
    "        output = tf.nn.relu(output)\n",
    "        output = self.conv_end(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNet(tf.keras.Model):\n",
    "    '''\n",
    "    https://github.com/NVIDIA/nv-wavenet/blob/master/pytorch/wavenet.py\n",
    "    Use Skip-Cut Method to accelerate output stack\n",
    "    Run 1D Conv over inputs\n",
    "    '''\n",
    "    def __init__(self, params):\n",
    "        super(WaveNet, self).__init__()\n",
    "\n",
    "        self.num_layers = len(params.dilations)\n",
    "        self.max_dilation = max(params.dilations)\n",
    "        self.residual_channels = params.residual_channels \n",
    "        self.skip_channels = params.skip_channels \n",
    "        self.quantization_channels = params.quantization_channels\n",
    "        \n",
    "        self.receptive_field = params.initial_filter_width + \\\n",
    "                               (params.filter_width - 1) * sum(params.dilations)\n",
    "        \n",
    "        self.dilate_layers = []\n",
    "        self.res_layers = []\n",
    "        self.skip_layers = []\n",
    "        \n",
    "        self.embed = tf.keras.layers.Embedding(self.quantization_channels, self.residual_channels)\n",
    "        self.conv_in = tf.keras.layers.Conv1D(self.residual_channels, params.initial_filter_width,\n",
    "                                              use_bias=False, activation='relu')\n",
    "        self.conv_out = tf.keras.layers.Conv1D(self.quantization_channels , 1, use_bias=False, activation='relu')\n",
    "        self.conv_end = tf.keras.layers.Conv1D(self.quantization_channels, 1, use_bias=False)\n",
    "\n",
    "        loop_factor = np.floor(np.log2(self.max_dilation)) + 1\n",
    "        for i in range(self.num_layers):\n",
    "            dilation = int(2 ** (i % loop_factor))\n",
    "            \n",
    "            # Kernel size is 2 in nv-wavenet\n",
    "            in_layer = tf.keras.layers.Conv1D(2 * params.residual_channels, kernel_size=2, padding='causal',\n",
    "                                              dilation_rate=dilation, use_bias=params.use_biases)\n",
    "            self.dilate_layers.append(in_layer)\n",
    "\n",
    "            # last one is not necessary\n",
    "            if i < self.num_layers - 1:\n",
    "                res_layer = tf.keras.layers.Conv1D(params.residual_channels, 1, use_bias=params.use_biases)\n",
    "                self.res_layers.append(res_layer)\n",
    "\n",
    "            skip_layer = tf.keras.layers.Conv1D(params.skip_channels, 1, use_bias=params.use_biases)\n",
    "\n",
    "            self.skip_layers.append(skip_layer)\n",
    "\n",
    "    def call(self, inp, mask):\n",
    "        output_width = int(inp.shape[1] - self.receptive_field + 1)\n",
    "\n",
    "        assert(np.max(inp) <= 1.)\n",
    "        assert(np.min(inp) >= 0.)\n",
    "\n",
    "        inp = self.embed(tf.cast(self.quantization_channels * (tf.squeeze(inp)), tf.int16))\n",
    "\n",
    "        inp = self.conv_in(inp)\n",
    "       \n",
    "        for i in range(self.num_layers):\n",
    "            in_act = self.dilate_layers[i](inp)\n",
    "            t_act = tf.tanh(in_act[:, :, :self.residual_channels])\n",
    "            s_act = tf.sigmoid(in_act[:, :, self.residual_channels:])\n",
    "            acts = t_act * s_act\n",
    "            if i < len(self.res_layers):\n",
    "                res_acts = self.res_layers[i](acts)\n",
    "            inp = res_acts + inp\n",
    "\n",
    "            if i == 0:\n",
    "                output = self.skip_layers[i](acts)[:,-output_width:]\n",
    "            else:\n",
    "                output = self.skip_layers[i](acts)[:,-output_width:] + output\n",
    "\n",
    "        output = tf.nn.relu(output)\n",
    "        output = self.conv_out(output)\n",
    "        output = tf.nn.relu(output)\n",
    "        output = self.conv_end(output)\n",
    "        \n",
    "        # Remove last probability and replace first value with 0's because we don't know\n",
    "\n",
    "        last = output[:, :, -1:]\n",
    "        output = output[:, :, :-1]\n",
    "        first = last * 0.0\n",
    "        output = tf.concat((first, output), 2)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WaveNet(hparams)\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate = hparams.learning_rate)\n",
    "optimizer = tf.train.RMSPropOptimizer(hparams.learning_rate)\n",
    "if True:\n",
    "    gaussian_filter = tf.keras.layers.Conv2D(1, kernel_size=(1,5), padding='same', use_bias=False,\n",
    "                        kernel_initializer = tf.initializers.constant([[.01, .05, .88, .05, .01]]))\n",
    "elif True:\n",
    "    gaussian_filter = tf.keras.layers.Conv2D(1, kernel_size=(1,3), padding='same', use_bias=False,\n",
    "                            kernel_initializer = tf.initializers.constant([[.01, .98, .01]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :  1.129 1.093 0.088\n",
      "1 :  1.131 1.077 0.088\n",
      "2 :  1.128 1.079 0.088\n",
      "3 :  1.13 1.076 0.088\n",
      "4 :  1.126 1.079 0.088\n",
      "5 :  1.129 1.075 0.088\n",
      "6 :  1.126 1.074 0.088\n",
      "7 :  1.127 1.074 0.088\n",
      "8 :  1.124 1.073 0.088\n",
      "9 :  1.126 1.073 0.088\n",
      "10 :  1.123 1.072 0.087\n",
      "11 :  1.125 1.072 0.087\n",
      "12 :  1.122 1.072 0.087\n",
      "13 :  1.124 1.071 0.087\n",
      "14 :  1.121 1.071 0.087\n",
      "15 :  1.123 1.071 0.087\n",
      "16 :  1.12 1.07 0.087\n",
      "17 :  1.123 1.07 0.087\n",
      "18 :  1.119 1.069 0.087\n",
      "19 :  1.121 1.069 0.087\n",
      "20 :  1.118 1.069 0.087\n",
      "21 :  1.121 1.068 0.087\n",
      "22 :  1.118 1.067 0.087\n"
     ]
    }
   ],
   "source": [
    "# Gaussian dilation embedded label\n",
    "ix = model.receptive_field + 1\n",
    "\n",
    "for epoch in range(100):\n",
    "    losses = []\n",
    "    for inp, target, mask, x_max, x_min in iter(dataset):\n",
    "        #if epoch % 3 != 0:\n",
    "        #   inp, target = jitter(inp, target)\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = model(inp[:,:-1], mask)\n",
    "\n",
    "            if epoch % 1 != 0:\n",
    "                # Apply Gaussian dilated loss\n",
    "                labels = create_gaussian_labels(inp[:,ix+1:])\n",
    "                ce_loss = tf.losses.softmax_cross_entropy(labels[:,:], pred[:,ix:])\n",
    "            else:\n",
    "                # Apply one-hot loss\n",
    "                ce_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    labels=tf.cast(hparams.quantization_channels * (inp[:,ix+1:]), tf.int32)[:,:,0],\n",
    "                    logits=pred[:, ix:])\n",
    "                ce_loss = tf.reduce_mean(ce_loss)\n",
    "            l2_loss = tf.reduce_sum([tf.reduce_sum(var ** 2) * (1 if var.name.find('bias') == -1 else 0)\\\n",
    "                                     for var in model.trainable_variables]) * hparams.lambd\n",
    "            loss = tf.reduce_sum([ce_loss, l2_loss])\n",
    "            \n",
    "        # Update gradients\n",
    "        variables = model.variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        losses.append(loss)\n",
    "    \n",
    "    #optimizer._learning_rate *= 1.02\n",
    "    if epoch % 1 == 0:\n",
    "        print(epoch, ': ', np.round(np.dstack(losses).mean(), 3), np.round(ce_loss, 3), np.round(l2_loss,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-shot train dataset\n",
    "# Designed for use with the whole dataset at once\n",
    "for epoch in range(100):\n",
    "    losses = []\n",
    "    inp, target, mask, x_max = next(iter(dataset))\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = model_(inp[:,:-2], mask)\n",
    "\n",
    "        if epoch % 2 == 0:\n",
    "            pred = model_(inp[:,:-2], mask)\n",
    "            pred = tf.log(tf.nn.softmax(pred))\n",
    "            pred0 = tf.random.multinomial(pred[0], 1)\n",
    "            pred1 = tf.random.multinomial(pred[1], 1)\n",
    "            pred = tf.stack([pred0, pred1]) / 500 + 0.85\n",
    "            pred = model_(pred, mask)\n",
    "\n",
    "        labels = create_gaussian_labels(inp[:,2:])\n",
    "        loss = tf.losses.softmax_cross_entropy(labels, pred)\n",
    "\n",
    "    # Update gradients\n",
    "    variables = model_.variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer_.apply_gradients(zip(gradients, variables))\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(epoch, ': ', np.dstack(losses).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot embedded label\n",
    "for epoch in range(2):\n",
    "    losses = []\n",
    "    for inp, target, mask, x_max in iter(dataset):\n",
    "        inp, target = jitter(inp, target)\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = model_(inp, mask)\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    labels=tf.cast(500 * (inp[:,127:] - 0.85), tf.int32)[:,:,0],\n",
    "                    logits=pred)\n",
    "            \n",
    "        # Update gradients\n",
    "        variables = model_.variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        optimizer_.apply_gradients(zip(gradients, variables))\n",
    "        losses.append(loss)\n",
    "\n",
    "    print(epoch, ': ', np.dstack(losses).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, target, mask, x_max, x_min = next(iter(dataset))\n",
    "\n",
    "forecast = inp[:8,:257]\n",
    "for _ in range(72):\n",
    "    pred = model(forecast[:,-257:], mask)\n",
    "    pred = tf.log(tf.nn.softmax(pred[:,-1]))\n",
    "    pred = tf.random.multinomial(pred, 1) / hparams.quantization_channels\n",
    "    #pred = tf.argmax(pred, -1) / (hparams.quantization_channels + 1)\n",
    "    pred = tf.reshape(tf.cast(pred, tf.float32), [-1,1,1])\n",
    "    forecast = tf.concat([forecast, pred], 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4710b154e0>"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.squeeze(inp[0,:330]) * np.max(x_max))\n",
    "#plt.plot(np.squeeze(np.concatenate([inp[0], inp[129]], axis=0)) * np.max(x_max))\n",
    "\n",
    "plt.plot(np.squeeze(forecast[:2]).T * np.max(x_max))\n",
    "plt.plot(np.convolve(forecast[1,:,0], [0.5, 0.5])[1:-1] * np.max(x_max))\n",
    "plt.plot(np.convolve(forecast[0,:,0], [0.5, 0.5])[1:-1] * np.max(x_max))\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "pred = model(inp[:,-129:], mask)\n",
    "plt.plot(np.squeeze(tf.nn.softmax(pred[0,-3:])).T)\n",
    "plt.show()\n",
    "plt.imshow(np.squeeze(tf.nn.softmax(pred))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Model\n",
    "\n",
    "loss_function: Build out MSE loss function with parameter regularisation\n",
    "\n",
    "train_model: Runs the minibatch training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNet(tf.keras.Model):\n",
    "    '''\n",
    "    https://github.com/NVIDIA/nv-wavenet/blob/master/pytorch/wavenet.py\n",
    "    Teacher forcing model\n",
    "    '''\n",
    "    def __init__(self, params):\n",
    "        super(WaveNet, self).__init__()\n",
    "\n",
    "        self.num_layers = len(params['dilations'])\n",
    "        self.max_dilation = max(params['dilations'])\n",
    "        self.residual_channels = params['residual_channels'] \n",
    "        self.skip_channels = params['skip_channels'] \n",
    "        self.out_channels = params['quantization_channels']\n",
    "        \n",
    "        self.receptive_field = params['initial_filter_width'] - 1 + \\\n",
    "                               (params['filter_width'] - 1) * sum(params['dilations'])\n",
    "        \n",
    "        self.dilate_layers = []\n",
    "        self.res_layers = []\n",
    "        self.skip_layers = []\n",
    "        \n",
    "        #self.embed = tf.keras.layers.Embedding(self.out_channels, self.residual_channels)\n",
    "        self.conv_out = tf.keras.layers.Conv1D(params['quantization_channels'] , 1, use_bias=False)\n",
    "        self.conv_end = tf.keras.layers.Conv1D(1, 1, use_bias=False)\n",
    "\n",
    "        loop_factor = np.floor(np.log2(self.max_dilation)) + 1\n",
    "        for i in range(self.num_layers):\n",
    "            dilation = int(2 ** (i % loop_factor))\n",
    "            \n",
    "            # Kernel size is 2 in nv-wavenet\n",
    "            in_layer = tf.keras.layers.Conv1D(2 * params['residual_channels'], kernel_size=2,\n",
    "                                              dilation_rate=dilation, use_bias=params['use_biases'])\n",
    "            self.dilate_layers.append(in_layer)\n",
    "\n",
    "            # last one is not necessary\n",
    "            if i < self.num_layers - 1:\n",
    "                res_layer = [tf.keras.layers.Conv1D(params['residual_channels'], 1, use_bias=params['use_biases']),\n",
    "                             tf.keras.layers.Conv1D(params['residual_channels'], kernel_size=2,\n",
    "                                              dilation_rate=dilation, use_bias=params['use_biases'])]\n",
    "                self.res_layers.append(res_layer)\n",
    "\n",
    "            if i == 0:\n",
    "                skip_layer = [tf.keras.layers.Conv1D(params['skip_channels'], 1, use_bias=params['use_biases'])]\n",
    "            else:\n",
    "                skip_layer = [tf.keras.layers.Conv1D(params['skip_channels'], 1, use_bias=params['use_biases']),\n",
    "                              tf.keras.layers.Conv1D(params['skip_channels'], kernel_size=2,\n",
    "                                              dilation_rate=dilation, use_bias=params['use_biases'])]\n",
    "\n",
    "            self.skip_layers.append(skip_layer)\n",
    "\n",
    "    def call(self, inp, mask):\n",
    "       \n",
    "        for i in range(self.num_layers):\n",
    "            in_act = self.dilate_layers[i](inp)\n",
    "            t_act = tf.tanh(in_act[:, :, :self.residual_channels])\n",
    "            s_act = tf.sigmoid(in_act[:, :, self.residual_channels:])\n",
    "            acts = t_act * s_act\n",
    "            if i < len(self.res_layers):\n",
    "                res_acts = self.res_layers[i][0](acts)\n",
    "                inp = res_acts + self.res_layers[i][1](inp)\n",
    "\n",
    "            if i == 0:\n",
    "                output = self.skip_layers[i][0](acts)\n",
    "            else:\n",
    "                output = self.skip_layers[i][0](acts) + self.skip_layers[i][1](output)\n",
    "\n",
    "        output = tf.nn.relu(output)\n",
    "        output = self.conv_out(output)\n",
    "        output = tf.nn.relu(output)\n",
    "        output = self.conv_end(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = WaveNet(params)\n",
    "optimizer_= tf.train.AdamOptimizer(learning_rate = hparams.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous Prediction\n",
    "for epoch in range(10):\n",
    "    losses = []\n",
    "    for inp, target, mask, x_max in iter(dataset):\n",
    "        inp, target = jitter(inp, target)\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = model_(inp[:,:-1], mask)\n",
    "            loss = tf.losses.mean_squared_error(inp[:,128:], pred)\n",
    "            \n",
    "        # Update gradients\n",
    "        variables = model_.variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        optimizer_.apply_gradients(zip(gradients, variables))\n",
    "        losses.append(loss)\n",
    "\n",
    "    print(epoch, ': ', np.dstack(losses).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = inp[:,:129]\n",
    "for _ in range(72):\n",
    "    pred = model_(forecast[:,-128:], mask)\n",
    "    forecast = tf.concat([forecast, pred], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.squeeze(forecast[:2]).T * np.max(x_max))\n",
    "plt.plot(np.squeeze(inp[0,:200]) * np.max(x_max))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.squeeze(pred).T * np.max(x_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.squeeze(inp[0::24, :72]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "models, stats = [], []\n",
    "\n",
    "lr    = np.log10(hparams.learning_rate)\n",
    "nodes = hparams.neurons_unit\n",
    "\n",
    "inp, target, mask, x_max = next(iter(dataset))\n",
    "\n",
    "#for lr in np.random.uniform(-8, -1, 10):\n",
    "#for nodes in np.floor(2 ** np.arange(3,9)):\n",
    "for lr in range(-8, 0, 1):\n",
    "    lr /= 2\n",
    "\n",
    "    # Run a training batch\n",
    "    tf.set_random_seed(231) # Set seed\n",
    "\n",
    "    # Initialise model and optimiser\n",
    "    model_ = WaveNet(params)\n",
    "    optimizer_ = tf.train.AdamOptimizer(learning_rate = 10 ** lr)\n",
    "\n",
    "    # Start training run\n",
    "    loss, accuracy, run_time, stat = \\\n",
    "        trainer.train_model(model_, optimizer_, dataset, hparams, epochs = 3, verbose=False)\n",
    "    print('Learning Rate {:.4f} Loss {:.4f} Accuracy {:.4f} Time {:.1f}'.format(lr, loss*10000, accuracy * 10000, run_time))\n",
    "    \n",
    "    models.append(model_)\n",
    "    stats.append(stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Rate -3.5000 Loss 0.0254 Accuracy 0.0254 Time 32.2\n",
    "Learning Rate -3.0000 Loss 0.0247 Accuracy 0.0247 Time 31.2\n",
    "Learning Rate -2.5000 Loss 0.0254 Accuracy 0.0254 Time 31.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, dataset_eval in([['Train', dataset], ['Test', dataset_test]]):\n",
    "    print(name + ' Results')\n",
    "    total_accuracy = []\n",
    "    x_variance = x_var(hparams, mode=get_fields)\n",
    "\n",
    "    for i, (inp, target, mask, x_max) in enumerate(dataset_eval):\n",
    "        forecast = model(inp, mask)\n",
    "\n",
    "        accuracy = tf.squeeze(forecast - target) * x_max\n",
    "        accuracy = accuracy ** 2 / x_variance\n",
    "        accuracy = np.mean(accuracy, axis=0)\n",
    "\n",
    "        total_accuracy.append(accuracy)\n",
    "\n",
    "    model_accuracy.append((hparams.in_seq_len, np.mean(total_accuracy, axis=0)))\n",
    "\n",
    "    print('Total RSE Accuracy: %.4f' % np.mean(total_accuracy))\n",
    "    print('Total MSE Accuracy: %.4f' % (np.mean(total_accuracy) * x_variance))\n",
    "    plt.plot(np.mean(total_accuracy, axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ibab/tensorflow-wavenet\n",
    "https://github.com/basveeling/wavenet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
